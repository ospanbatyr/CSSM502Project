{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbd9ba3-d10a-4fed-b55f-6c35eb923afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/obi/opt/miniconda3/envs/dl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import textacy\n",
    "import textacy.preprocessing as tprep\n",
    "import random, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "from collections import Counter\n",
    "import keras_tuner\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras import layers\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3ce6ea-0777-42fd-af4c-16ba491d0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "# Seed everything to make results reproducible\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5a9ad-adf5-4331-8f80-0f2f270569fc",
   "metadata": {},
   "source": [
    "## Main Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901bae60-e890-4b9d-82ad-8d4b7fcc17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the full dataset\n",
    "file_path = \"dataset.tsv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76a2e44-6f76-4ad5-bea8-c61c62a72910",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [tprep.remove.accents(word) for word in stopwords.words(\"turkish\")]\n",
    "\n",
    "# Tokenize the Turkish text so that it is ready to be used as features\n",
    "def tokenize(text):\n",
    "    text = text.lower()                                                           # lowercase the text\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))              # remove punctuation\n",
    "    text = tprep.normalize.unicode(text)                                          # normalize unicode\n",
    "    text = tprep.remove.accents(text)                                             # remove accents\n",
    "    text = word_tokenize(text, language='turkish')                                # split the text into words\n",
    "    text = [word for word in text if not word in stopwords]                       # remove unnecessary noise words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aed2b8d-a668-4d84-99c8-c9c2160b6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple tokenizer class to preprocess tokenization to the Bi-LSTM model\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, counter: Counter, max_features: int, max_length: int = 50):\n",
    "        # the word count for the input text\n",
    "        self.counter = counter\n",
    "        self.max_features = max_features\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # only use the most common features, others are most probably noise (excluding stopwords)\n",
    "        most_common_tokens = [pair[0] for pair in counter.most_common(max_features)]\n",
    "        self.w2i = {token:(i+1) for i, token in enumerate(most_common_tokens)}\n",
    "        self.i2w = {(i+1):token for i, token in enumerate(most_common_tokens)}\n",
    "        self.pad_id = 0\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()                                                           # lowercase the text\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))              # remove punctuation\n",
    "        text = tprep.normalize.unicode(text)\n",
    "        text = tprep.remove.accents(text)\n",
    "        text = word_tokenize(text, language='turkish')                                # split the text into words\n",
    "        text = [word for word in text if not word in stopwords] # remove unnecessary noise words\n",
    "        return text\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        # first tokenize the text with some preprocessing\n",
    "        tokens = self.tokenize(text)\n",
    "        token_ids = []\n",
    "\n",
    "        # if the token is in the vocabulary, add it to the token_ids list\n",
    "        for token in tokens:\n",
    "            if token in self.w2i:\n",
    "                token_ids.append(self.w2i[token])\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # decode the token ids back to the text\n",
    "        tokens = []\n",
    "        for token_id in ids:\n",
    "            if token_id != self.pad_id:\n",
    "                tokens.append(self.i2w[token_id])\n",
    "\n",
    "        return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45d1aee-1e14-4b4a-a3ff-5889bd76d977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a simple dataset class to preprocess the data to the Bi-LSTM model\n",
    "tokens = [tokenize(c) for c in df[\"Comment\"].tolist()]\n",
    "tokens = Counter(list(chain(*tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c92eebf-3096-4bbd-8497-f509a8325834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max_features to 5000 to use the most common 5000 words in the dataset\n",
    "max_features = 5000\n",
    "maxlen = 50\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5decd2-d84b-489f-bac5-9eed9a83ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer(tokens, (max_features - 1), maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7eb85fa-6493-4bb6-afde-409cc8014012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 40450/40450 [00:02<00:00, 16288.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the input data by tokenizing the comments\n",
    "\n",
    "X = []\n",
    "for comment in tqdm(df[\"Comment\"].tolist()):\n",
    "    X.append(tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a673fa-9bd9-47a6-9043-fab1886ddcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scores with a mapping due to indexing scheme in Python\n",
    "mapping = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a186c70b-611e-416e-ba63-1c338176c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output data by mapping the scores\n",
    "y = df[\"Score\"].apply(lambda x: mapping[x]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb38c8f6-875d-4e6d-85bf-4b668f5635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e605a5bf-7e29-42d5-9af0-e7f0e32b2165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to the same length\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9345ceb0-6614-4cb7-952a-442944dc2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scores to categorical data\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24682778-140c-49c1-a615-0da8d3524eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bi-LSTM model\n",
    "# The model is created with Keras Tuner to find the best hyperparameters for the model\n",
    "\n",
    "def build_model(hp):\n",
    "    # Metrics to be used in the model\n",
    "    p = keras.metrics.Precision()\n",
    "    r = keras.metrics.Recall()\n",
    "    accuracy = keras.metrics.CategoricalAccuracy\n",
    "\n",
    "    # Hyperparameters to be tuned are the embedding dimension, hidden dimension, learning rate and weight decay\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Embedding(max_features, hp.Choice('embedding_dim', [64, 128])))\n",
    "    lstm_dim = hp.Choice('hidden_dim', [64, 128])\n",
    "    model.add(layers.Bidirectional(layers.LSTM(lstm_dim, return_sequences=True)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(lstm_dim)))\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 3e-4])\n",
    "    hp_weight_decay = hp.Choice('weight_decay', values=[5e-5, 1e-5, 5e-6])\n",
    "    model.compile(optimizer=keras.optimizers.AdamW(learning_rate=hp_learning_rate, weight_decay=hp_weight_decay), loss=\"categorical_crossentropy\", metrics=[accuracy, p, r])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27f8d3f5-d1b5-4b30-9195-63a9634510b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ./untitled_project/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Create the tuner\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_categorical_accuracy',\n",
    "    max_trials=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092b6c09-836a-4934-9915-76c0d70394b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search\n",
    "tuner.search(x_train, y_train, epochs=2, validation_split=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d861c8-832c-4ce4-9a0e-62dffe96748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/obi/opt/miniconda3/envs/dl/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:388: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 32 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Get the best model\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5671f227-508f-4bdd-ba9f-8ff2c2679d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - categorical_accuracy: 0.8271 - loss: 0.5339 - precision: 0.8599 - recall: 0.8035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5304042100906372,\n",
       " 0.8253399133682251,\n",
       " 0.8596793413162231,\n",
       " 0.8019777536392212]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the best model\n",
    "best_model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0fc1639-f0e0-4b76-a661-15419a684d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions for further analysis\n",
    "\n",
    "predictions = best_model.predict(x_val)\n",
    "preds = predictions.argmax(axis=-1)\n",
    "golds = y_val.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebd82362-6ee9-45c8-81ea-d7170e994fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69       381\n",
      "           1       0.00      0.00      0.00       113\n",
      "           2       0.33      0.03      0.05       272\n",
      "           3       0.45      0.07      0.13       937\n",
      "           4       0.85      0.99      0.91      6387\n",
      "\n",
      "    accuracy                           0.83      8090\n",
      "   macro avg       0.45      0.38      0.36      8090\n",
      "weighted avg       0.76      0.83      0.77      8090\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 307    0    4    2   68]\n",
      " [  67    0    3    3   40]\n",
      " [  55    3    8   43  163]\n",
      " [  29    3    4   69  832]\n",
      " [  53    1    5   35 6293]]\n",
      "\n",
      "Precision: 76.38\n",
      "Recall: 82.53\n",
      "F1 Score: 76.98\n",
      "Accuracy: 82.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "report = classification_report(golds, preds, output_dict=True)\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1 = report['weighted avg']['f1-score']\n",
    "\n",
    "# Display the classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\\n\", classification_report(golds, preds))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(golds, preds))\n",
    "print(\"\\nPrecision:\", f\"{(precision * 100):.2f}\")\n",
    "print(\"Recall:\", f\"{(recall * 100):.2f}\")\n",
    "print(\"F1 Score:\", f\"{(f1 * 100):.2f}\")\n",
    "print(\"Accuracy:\", f\"{(accuracy_score(golds, preds) * 100):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c69dfc46-15e5-4e49-af90-aeffc7b2ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions for statitical significance test\n",
    "with open(\"bilstm_preds.npy\", \"wb\") as f:\n",
    "    np.save(f, preds + 1)\n",
    "\n",
    "# Save the gold labels for statitical significance test\n",
    "with open(\"bilstm_golds.npy\", \"wb\") as f:\n",
    "    np.save(f, golds + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74281fce-7b3a-47f0-89b1-9f311ad4a1c4",
   "metadata": {},
   "source": [
    "## OOD Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60f0e97b-1712-43c4-8271-c10fa04a7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the OOD dataset\n",
    "file_path = \"OsmanBaturInce_ood_dataset.tsv\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=[\"Comment\", \"Score\", \"Link\", \"Brand\", \"Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a9567a9-62dc-423a-b587-d29850dc3a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1182/1182 [00:00<00:00, 15452.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Tokenize the OOD dataset to create the feature matrix\n",
    "\n",
    "X = []\n",
    "for comment in tqdm(df[\"Comment\"].tolist()):\n",
    "    X.append(tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "684b2148-89f7-4846-922d-525ad2b9ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output data by mapping the scores and converting them to categorical data\n",
    "y = df[\"Score\"].apply(lambda x: mapping[x]).to_numpy()\n",
    "y = keras.utils.to_categorical(y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d225263f-1cc9-4d20-af77-63bb1d81527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to the same length\n",
    "X = keras.utils.pad_sequences(X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7495b347-894f-418a-9112-95ee59fca027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - categorical_accuracy: 0.8451 - loss: 0.4891 - precision: 0.8724 - recall: 0.8267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49778124690055847,\n",
       " 0.8443316221237183,\n",
       " 0.8727436661720276,\n",
       " 0.8181049227714539]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the best model on the OOD dataset\n",
    "best_model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac03d4bf-4617-4fc8-99a4-3921e86831b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions for further analysis\n",
    "predictions = best_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cdeee4d-3284-4d53-8941-057e04d784d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51f93147-5001-4233-8a52-f4c6f7b21eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "golds = y.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e471b86-07ec-4edd-8f9b-a6b00ed1fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.64      0.33        11\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.20      0.02      0.03       117\n",
      "           4       0.87      0.98      0.92      1012\n",
      "\n",
      "    accuracy                           0.84      1182\n",
      "   macro avg       0.26      0.33      0.26      1182\n",
      "weighted avg       0.76      0.84      0.79      1182\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  7   0   0   0   4]\n",
      " [  1   0   0   0   5]\n",
      " [  3   0   0   3  30]\n",
      " [  3   0   0   2 112]\n",
      " [ 18   0   0   5 989]]\n",
      "\n",
      "Precision: 76.46\n",
      "Recall: 84.43\n",
      "F1 Score: 79.31\n",
      "Accuracy: 84.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/obi/opt/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/obi/opt/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/obi/opt/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/obi/opt/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/obi/opt/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/obi/opt/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "report = classification_report(golds, preds, output_dict=True)\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1 = report['weighted avg']['f1-score']\n",
    "\n",
    "# Display the classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\\n\", classification_report(golds, preds))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(golds, preds))\n",
    "print(\"\\nPrecision:\", f\"{(precision * 100):.2f}\")\n",
    "print(\"Recall:\", f\"{(recall * 100):.2f}\")\n",
    "print(\"F1 Score:\", f\"{(f1 * 100):.2f}\")\n",
    "print(\"Accuracy:\", f\"{(accuracy_score(golds, preds) * 100):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c15c0121-263b-48dd-946a-0828d006a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions for statitical significance test\n",
    "with open(\"bilstm_ood_preds.npy\", \"wb\") as f:\n",
    "    np.save(f, preds + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f6c3bc3-77bf-4606-9e78-7ac3ad7cd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the gold labels for statitical significance test\n",
    "with open(\"bilstm_ood_golds.npy\", \"wb\") as f:\n",
    "    np.save(f, golds + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a19c87-1c44-4037-b188-35a16ac68e1c",
   "metadata": {},
   "source": [
    "## Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a1ce11e-0171-46a0-af3d-5ace34492479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from pprint import pprint\n",
    "\n",
    "# McNemar's Test for statistical significance\n",
    "# The test is performed on the predictions of every pair of models\n",
    "def mcnemar_test(model1_preds, model2_preds, model1_golds, model2_golds):\n",
    "    table = [[0, 0], [0, 0]]\n",
    "\n",
    "    for pred1, pred2, gold in zip(model1_preds, model2_preds, model1_golds):\n",
    "        if pred1 == gold and pred2 == gold:\n",
    "            continue\n",
    "        elif pred1 == gold and pred2 != gold:\n",
    "            table[0][1] += 1\n",
    "        elif pred1 != gold and pred2 == gold:\n",
    "            table[1][0] += 1\n",
    "        elif pred1 != gold and pred2 != gold:\n",
    "            table[1][1] += 1\n",
    "\n",
    "    if table[0][1] + table[1][0] < 50:\n",
    "        exact = True\n",
    "    else:\n",
    "        exact = False\n",
    "\n",
    "    result = mcnemar(table, exact=exact)\n",
    "    print(\"McNemar's Test Statistic:\", result.statistic)\n",
    "    print(\"P-value:\", result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68294d11-5d42-4266-80ca-075a1a9a94b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOD Random Forest - Bi-LSTM\n",
      "McNemar's Test Statistic: 6.0\n",
      "P-value: 0.009355306625366211\n",
      "\n",
      "OOD Random Forest - Naive Bayes\n",
      "McNemar's Test Statistic: 16.0\n",
      "P-value: 0.735878800856881\n",
      "\n",
      "OOD Bi-LSTM - Naive Bayes\n",
      "McNemar's Test Statistic: 14.0\n",
      "P-value: 0.10812902140605732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_vals = {\"preds\": np.load(\"rf_ood_preds.npy\"), \"golds\": np.load(\"rf_ood_golds.npy\")}\n",
    "bilstm_vals = {\"preds\": np.load(\"bilstm_ood_preds.npy\"), \"golds\": np.load(\"bilstm_ood_golds.npy\")} \n",
    "nb_vals = {\"preds\": np.load(\"nb_ood_preds.npy\"), \"golds\": np.load(\"nb_ood_golds.npy\")} \n",
    "\n",
    "vals = [rf_vals, bilstm_vals, nb_vals]\n",
    "names = [\"Random Forest\", \"Bi-LSTM\", \"Naive Bayes\"]\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# For every pair of models, perform the McNemar's Test for OOD predictions\n",
    "for model1, model2 in combinations(list(zip(vals, names)), 2):\n",
    "    model1_vals, model1_name = model1\n",
    "    model2_vals, model2_name = model2\n",
    "\n",
    "    model1_preds, model1_golds = model1_vals[\"preds\"], model1_vals[\"golds\"]\n",
    "    model2_preds, model2_golds = model2_vals[\"preds\"], model2_vals[\"golds\"]\n",
    "    print(\"OOD\", model1_name, \"-\", model2_name)\n",
    "    mcnemar_test(model1_preds, model2_preds, model1_golds, model2_golds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cb63c59-4d31-4308-abfb-59c56dae2428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IID Random Forest - Bi-LSTM\n",
      "McNemar's Test Statistic: 10.460251046025105\n",
      "P-value: 0.0012197070113041527\n",
      "\n",
      "IID Random Forest - Naive Bayes\n",
      "McNemar's Test Statistic: 0.8634361233480177\n",
      "P-value: 0.35277890006264423\n",
      "\n",
      "IID Bi-LSTM - Naive Bayes\n",
      "McNemar's Test Statistic: 5.020491803278689\n",
      "P-value: 0.025049053512565384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_vals = {\"preds\": np.load(\"rf_preds.npy\"), \"golds\": np.load(\"rf_golds.npy\")}\n",
    "bilstm_vals = {\"preds\": np.load(\"bilstm_preds.npy\"), \"golds\": np.load(\"bilstm_golds.npy\")} \n",
    "nb_vals = {\"preds\": np.load(\"nb_preds.npy\"), \"golds\": np.load(\"nb_golds.npy\")} \n",
    "\n",
    "vals = [rf_vals, bilstm_vals, nb_vals]\n",
    "names = [\"Random Forest\", \"Bi-LSTM\", \"Naive Bayes\"]\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# For every pair of models, perform the McNemar's Test for IID predictions\n",
    "for model1, model2 in combinations(list(zip(vals, names)), 2):\n",
    "    model1_vals, model1_name = model1\n",
    "    model2_vals, model2_name = model2\n",
    "\n",
    "    model1_preds, model1_golds = model1_vals[\"preds\"], model1_vals[\"golds\"]\n",
    "    model2_preds, model2_golds = model2_vals[\"preds\"], model2_vals[\"golds\"]\n",
    "    print(\"IID\", model1_name, \"-\", model2_name)\n",
    "    mcnemar_test(model1_preds, model2_preds, model1_golds, model2_golds)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
